version: 1
name: llama_block
notes: "Minimal Llama block expressed as Residual([Norm, Attention]) + Residual([Norm, SwiGLU])."
vars:
  d_model: 128
  n_heads: 4
  n_kv_heads: 4
  head_dim: 32  # d_model / n_heads
  d_ff: 256
  n_blocks: 2
defaults:
  tokenizer: tiktoken
  val_frac: 0.1
  instrument: rich
  wandb: false
  wandb_project: ""
  wandb_entity: ""
  wandb_mode: offline
  eval_iters: 20
  save_every: 0
model:
  type: transformer
  topology:
    type: nested
    repeat: "${n_blocks}"
    layers:
      - type: residual
        layers:
          - type: rms_norm
            d_model: "${d_model}"
            eps: 1e-5
          - type: attention
            d_model: "${d_model}"
            n_q_heads: "${n_heads}"
            n_kv_heads: "${n_kv_heads}"
            head_dim: "${head_dim}"
            is_causal: true
            dropout_p: 0.0
      - type: residual
        layers:
          - type: rms_norm
            d_model: "${d_model}"
            eps: 1e-5
          - type: swiglu
            d_model: "${d_model}"
            d_ff: "${d_ff}"
            bias: false
groups:
  - name: default
    description: Default group
    data: ""
    runs:
      - id: default
        mode: train
        exp: llama_block
        seed: 1337
        steps: 1000
        expected: {}
