version: 1
name: llama_block
notes: "Minimal Llama block expressed as Residual([Norm, Attention]) + Residual([Norm, SwiGLU])."
vars:
  d_model: 128
  n_heads: 4
  n_kv_heads: 4
  rope_base: 10000.0
  rope_dim: 32
  d_ff: 256
  n_blocks: 2
defaults:
  tokenizer: tiktoken
  val_frac: 0.1
  instrument: rich
  wandb: false
  wandb_project: ""
  wandb_entity: ""
  wandb_mode: offline
  eval_iters: 20
  save_every: 0
model:
  type: transformer
  topology:
    type: nested
    repeat: "${n_blocks}"
    layers:
      - type: residual
        layers:
          - type: rms_norm
            operation: { type: rms_norm, eps: 1e-5 }
            weight: { type: rms_norm, d_model: "${d_model}" }
          - type: attention
            operation: { type: attention, is_causal: true, dropout_p: 0.0 }
            weight:
              type: llama_attention
              d_model: "${d_model}"
              n_heads: "${n_heads}"
              n_kv_heads: "${n_kv_heads}"
              rope_base: "${rope_base}"
              rope_dim: "${rope_dim}"
              bias: false
      - type: residual
        layers:
          - type: rms_norm
            operation: { type: rms_norm, eps: 1e-5 }
            weight: { type: rms_norm, d_model: "${d_model}" }
          - type: swiglu
            operation: { type: swiglu }
            weight:
              type: swiglu
              d_model: "${d_model}"
              d_ff: "${d_ff}"
              bias: false
groups:
  - name: default
    description: Default group
    data: ""
    runs:
      - id: default
        mode: train
        exp: llama_block
        seed: 1337
        steps: 1000
        expected: {}

