version: 1
name: llama32_1b_dba_compare
notes: "Llama 3.2 1B with DBA attention via upcycle + post-run compare verify."
vars:
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  # DBA dimensions - aggressive compression (~5.3x KV-cache reduction)
  sem_dim: 128   # Semantic bottleneck
  geo_dim: 256   # Geometric path with RoPE
defaults:
  tokenizer: llama
  val_frac: 0.05
  instrument: rich
  wandb: true
  wandb_project: "dba-upcycle"
  wandb_entity: ""
  wandb_mode: online
  eval_iters: 50
  save_every: 500
model:
  type: TransformerModel
  embedder:
    type: token
    vocab_size: "${vocab_size}"
    d_model: "${d_model}"
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: "${n_layers}"
        layers:
          # Attention residual block with DBA
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: "${d_model}"
                eps: 1e-5
              - type: AttentionLayer
                d_model: "${d_model}"
                n_heads: "${n_heads}"
                n_kv_heads: "${n_kv_heads}"
                mode: decoupled
                sem_dim: "${sem_dim}"
                geo_dim: "${geo_dim}"
                rope_enabled: true
                rope_base: 500000.0
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
          # MLP residual block
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: "${d_model}"
                eps: 1e-5
              - type: SwiGLULayer
                d_model: "${d_model}"
                d_ff: "${d_ff}"
                bias: false
      # Final norm
      - type: RMSNormLayer
        d_model: "${d_model}"
        eps: 1e-5
      # LM head
      - type: LinearLayer
        d_in: "${d_model}"
        d_out: "${vocab_size}"
        bias: false
groups:
  - name: upcycle
    description: "DBA upcycle from Llama 3.2 1B, with post-run compare."
    data: "fineweb_100m.npy"
    runs:
      - id: blockwise
        mode: train
        exp: dba_blockwise
        seed: 42
        steps: 500
        expected:
          phase: blockwise
        verify:
          type: compare
          batches: 2
          attention:
            max_mean_l1: 0.05
            max_max_l1: 0.25
          logits:
            max_mean_l1: 0.05
            max_max_l1: 0.25
        train:
          phase: blockwise
          batch_size: 1
          block_size: 2048
          lr: 0.0001
          device: mps
          dtype: float32
          teacher_ckpt: "hf://meta-llama/Llama-3.2-1B"
      - id: finetune
        mode: train
        exp: dba_finetune
        seed: 42
        steps: 2000
        expected:
          phase: global
        train:
          phase: global
          batch_size: 1
          block_size: 2048
          lr: 0.00005
          device: mps
          dtype: float32
    benchmarks:
      - id: riddles
        repeats: 10
      - id: adversarial
        repeats: 10
