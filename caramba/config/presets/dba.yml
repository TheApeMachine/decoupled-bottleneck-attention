version: 1
name: dba
notes: "Decoupled Bottleneck Attention expressed via composable blocks."
vars:
  d_model: 128
  n_heads: 4
  n_kv_heads: 4
  sem_dim: 32   # Semantic key dimension (content path) - BOTTLENECK (routing is low-rank)
  geo_dim: 64   # Geometric key dimension (position path) - larger for RoPE fidelity
  d_ff: 256
  n_blocks: 2
defaults:
  tokenizer: tiktoken
  val_frac: 0.1
  instrument: rich
  wandb: true
  wandb_project: ""
  wandb_entity: ""
  wandb_mode: online
  eval_iters: 20
  save_every: 0
model:
  type: TransformerModel
  topology:
    type: NestedTopology
    repeat: "${n_blocks}"
    layers:
      - type: ResidualTopology
        layers:
          - type: RMSNormLayer
            d_model: "${d_model}"
            eps: 1e-5
          - type: AttentionLayer
            d_model: "${d_model}"
            n_heads: "${n_heads}"
            n_kv_heads: "${n_kv_heads}"
            mode: decoupled
            sem_dim: "${sem_dim}"
            geo_dim: "${geo_dim}"
            rope_enabled: true
            is_causal: true
            dropout_p: 0.0
            decoupled_gate: true
      - type: ResidualTopology
        layers:
          - type: RMSNormLayer
            d_model: "${d_model}"
            eps: 1e-5
          - type: SwiGLULayer
            d_model: "${d_model}"
            d_ff: "${d_ff}"
            bias: false
groups:
  - name: default
    description: Default group
    data: ""
    runs:
      - id: default
        mode: train
        exp: dba
        seed: 1337
        steps: 1000
        expected:
          attn_mode: decoupled
