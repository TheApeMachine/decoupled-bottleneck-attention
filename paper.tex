\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}  % Better text flow, reduces overfull hbox
\usepackage[margin=1in]{geometry}  % Consistent margins

% Custom colors
\definecolor{accent}{RGB}{45, 106, 79}

\title{\textbf{Decoupled Bottleneck Attention:} \\[0.3em] 
\large Scaling Efficient Transformers via Low-Rank Semantic Routing}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research}\\
  \texttt{theapemachine@gmail.com}
}

\date{December 2025}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
The Key-Value cache in Transformer models scales linearly with sequence length and model dimension, creating a critical memory bottleneck for long-context inference. While techniques like Grouped-Query Attention (GQA) reduce cache size by sharing key-value heads, they preserve the full computational cost of attention scoring in high-dimensional space.

We propose \textbf{Decoupled Bottleneck Attention}, an architectural modification that exploits the empirical observation that \textit{semantic routing}---deciding which tokens attend to which---operates in a low-rank subspace ($r \approx 32$), while \textit{positional geometry} requires higher fidelity ($r \approx 64$). By decoupling these concerns into separate projection paths, we achieve:
\begin{itemize}
    \item \textbf{Up to 168$\times$ end-to-end KV-cache reduction} in a fixed-rank long-context scenario, where $\sim 42.7\times$ comes from architectural dimension reduction and an additional $4\times$ comes from standard 4-bit quantization (Section~2.6)
    \item \textbf{Evidence at two scales:} v21 WikiText-2 ablations show that bottlenecking attention can outperform a full-rank baseline; v29 FineWeb-Edu scaling runs show that decoupling (48/96) outperforms a standard baseline, GQA (kv=2), and a matched-rank bottleneck baseline (Table~\ref{tab:fineweb})
    \item \textbf{Superior data efficiency compared to Grouped-Query Attention (GQA).} On FineWeb-Edu, our Decoupled Bottleneck model ($d_{\text{sem}}{=}48, d_{\text{geo}}{=}96$; total $d_{\text{attn}}=144$) achieves lower validation loss than a GQA baseline with shared KV heads (kv=2) under the same training recipe (mean over two seeds: $5.851 \pm 0.038$ vs $6.259 \pm 0.062$), demonstrating that reducing the \textit{interaction rank} is a more effective compression strategy than head sharing.
\end{itemize}

In our v21 WikiText-2 suite, a simple rank-96 bottleneck \textit{outperforms} the full rank-512 baseline (val loss 5.33 vs 5.37), suggesting that standard Transformers can over-allocate capacity to attention. In our v29 FineWeb-Edu suite, the Decoupled Bottleneck model achieves the best validation loss among baseline, GQA, and a matched-rank Bottleneck baseline (Table~\ref{tab:fineweb}).

\end{abstract}

\paragraph{Keywords:}
Transformer, attention mechanism, low-rank, KV-cache, memory efficiency, quantization, long context, rotary position embeddings

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Modern Transformer architectures \cite{vaswani2017attention} achieve remarkable performance across language modeling, translation, and reasoning tasks. However, their quadratic attention complexity and linear KV-cache growth present fundamental scalability challenges for long-context applications.

\subsection{The Redundancy Hypothesis}

We begin with a simple observation: in a 512-dimensional layer, the neurons are not independent. They move in \textit{sympathetic clusters}---correlated groups that reduce the intrinsic dimensionality of the representation. Prior work on LoRA \cite{hu2021lora} demonstrated that weight \textit{updates} during fine-tuning are low-rank (typically $r \leq 64$). Recent work on gradient dynamics \cite{refael2024adarankgrad} shows that optimization naturally collapses to low rank. We extend this observation to argue that the \textit{architecture itself}---specifically the attention mechanism---should be structurally constrained to match this intrinsic rank.

Empirical measurements from our experiments show that the effective rank of $W_Q$ and $W_K$ projections stabilizes around 11-32 dimensions, even when the nominal dimension is 512. This aligns with theoretical analysis by Bhojanapalli et al. \cite{bhojanapalli2020lowrank}, who identified a ``low-rank bottleneck'' in multi-head attention, and recent work by Kobayashi et al. \cite{kobayashi2024weightdecay} showing that weight decay actively induces rank reduction during training. Wang et al. \cite{wang2025lowdim} further demonstrate that attention outputs are approximately 60\% low-dimensional, adding to low-dimensional residual subspaces. Crucially, Refael et al. \cite{refael2024adarankgrad} proved that gradient rank \textit{decreases monotonically} during training, asymptotically approaching rank one---providing theoretical justification for why architectural bottlenecks become increasingly appropriate as training progresses. Chiang \& Yogatama \cite{chiang2025rope} show that RoPE may cause dimension inefficiency for long-distance retrieval, supporting our use of higher dimensions (64) for the geometric path.

\subsection{Comparison with Existing Approaches}

\paragraph{Grouped-Query Attention (GQA).}
While Grouped-Query Attention \cite{ainslie2023gqa} successfully reduces KV-cache memory by sharing key-value heads across multiple query heads, it maintains the full computational cost of the query projection and attention scoring in the high-dimensional space. Each query still operates in $\mathbb{R}^{d}$, and every attention score still requires a $d$-dimensional dot product---GQA merely amortizes the \textit{storage} cost, not the \textit{interaction} cost.

Our Bottleneck approach reduces both memory \textit{and} compute by compressing the interaction manifold. Rather than sharing high-dimensional KV pairs, we project queries and keys into a low-rank semantic subspace ($r \ll d$) \textit{before} computing attention, reducing dot-product complexity from $O(n^2 d)$ to $O(n^2 r)$.

\paragraph{Multi-Head Latent Attention (MLA).}
DeepSeek-V2 \cite{deepseek2024v2} introduced MLA, which compresses KV storage into a latent vector, achieving 93\% cache reduction. However, MLA \textit{up-projects} during the forward pass to perform attention in the original high-dimensional space. Our method remains low-rank throughout, saving both memory and compute.

\paragraph{Disentangled Attention.}
DeBERTa \cite{he2020deberta} pioneered the separation of content and position representations in attention scoring. We adopt this disentanglement principle but leverage it for \textit{efficiency}: applying aggressive compression to the semantic (content) path while preserving fidelity in the geometric (position) path.

\subsection{Contributions}

\begin{enumerate}
    \item We demonstrate that attention routing can be performed in $\sim$32 dimensions without perplexity degradation, while positional encoding requires $\sim$64 dimensions for RoPE fidelity.
    \item We propose \textbf{Decoupled Bottleneck Attention}, which separates semantic and geometric scoring paths with asymmetric dimensionality.
    \item We introduce a \textbf{Null Token} mechanism that stabilizes training by providing an explicit ``attend nowhere'' option.
    \item We show that architectural dimension reduction yields large KV-cache savings, and when combined with standard 4-bit quantization can reach \textbf{up to 168$\times$} end-to-end KV-cache compression in a fixed-rank scenario (Figure~\ref{fig:memory}).
\end{enumerate}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

\paragraph{Low-rank and approximate attention.}
A long line of work seeks to reduce the quadratic cost of attention by approximating the score computation or constraining its rank. Linformer \cite{wang2020linformer} projects keys and values into a lower-dimensional subspace along the sequence dimension, yielding linear-time attention under a low-rank assumption. Kernel and hashing methods such as Performer \cite{choromanski2021performer} and Reformer \cite{kitaev2020reformer} similarly reduce attention cost via randomized features or locality-sensitive hashing. Our setting is different: we train standard causal LMs, but explicitly reduce the query/key interaction dimension inside each layer, targeting both compute (\(O(n^2 r)\)) and KV-cache memory (\(O(n r)\)).

\paragraph{Sparse/local attention for long documents.}
Sparse patterns (e.g., sliding window with global tokens) as in Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce attention compute while retaining access to distant context. However, for autoregressive decoding these methods still accumulate a KV cache whose size grows linearly with context length. Our work instead reduces the per-token cache footprint, which is complementary to sparse attention and other long-context strategies \cite{huang2023longcontextsurvey}.

\paragraph{KV-cache optimization.}
Sharing KV heads reduces cache storage by amortizing keys and values across query heads (MQA/GQA) \cite{shazeer2019mqa,ainslie2023gqa}. Latent KV schemes such as MLA compress the cache into a lower-dimensional latent that is expanded during attention \cite{deepseek2024v2}. Orthogonally, quantizing the KV cache reduces memory at fixed architecture \cite{hooper2024kvquant,li2025commvq}. Our decoupled bottleneck reduces the interaction dimension before scoring (saving compute) and also makes heterogeneous KV quantization natural: semantic keys can often be quantized more aggressively than geometric keys.

\paragraph{Expressiveness limits and structured alternatives.}
Reducing interaction rank too far can harm representation power: theory and empirical evidence show regimes where increasing heads under fixed head dimension does not recover lost capacity \cite{bhojanapalli2020lowrank,amsel2025qualityheads}. Recent structured-matrix formulations aim to increase effective rank without full cost by parameterizing attention maps with richer structured operators \cite{kuang2025structuredmatrices}. Decoupling is a simple architectural compromise: we keep a higher-dimensional geometric path (with RoPE) while aggressively compressing only the semantic routing path.

% ============================================================================
% 2. METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Standard Multi-Head Attention}

In standard scaled dot-product attention with $H$ heads:
\begin{equation}
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{equation}
where $Q, K, V \in \mathbb{R}^{n \times d}$ are obtained by linear projection from the input $X \in \mathbb{R}^{n \times d_{\text{model}}}$:
\begin{equation}
    Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\end{equation}
with $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d}$. For language modeling with context length $n$ and dimension $d$, the KV-cache requires $O(2 \cdot L \cdot n \cdot d)$ memory, where $L$ is the number of layers.

\subsection{Bottleneck Attention}

We introduce a simple modification: project $Q$ and $K$ to a lower-dimensional space \textit{before} computing attention scores.\footnote{Our use of ``bottleneck'' refers to dimensionality reduction in the query/key space, distinct from Park et al.'s BAM \cite{park2018bam}, which applies channel and spatial attention in CNNs for computer vision.}
\begin{equation}
    Q' = XW_Q', \quad K' = XW_K'
\end{equation}
where $W_Q', W_K' \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}$ with $d_{\text{attn}} \ll d_{\text{model}}$. The attention computation becomes:
\begin{equation}
    \text{Attn}_{\text{bottleneck}}(Q', K', V') = \text{softmax}\left(\frac{Q'K'^\top}{\sqrt{d_{\text{attn}}/H}}\right) V'
\end{equation}

This reduces the dot-product complexity from $O(n^2 \cdot d_{\text{model}})$ to $O(n^2 \cdot d_{\text{attn}})$ and the KV-cache from $O(n \cdot d_{\text{model}})$ to $O(n \cdot d_{\text{attn}})$.

\subsection{Decoupled Bottleneck Attention}

The key insight motivating decoupling is that \textit{semantic matching} (``is this token semantically related?'') and \textit{geometric positioning} (``how far away is this token?'') have different intrinsic dimensionality requirements.

We decompose the attention score into two additive components:
\begin{equation}
    \text{Score} = \underbrace{\frac{Q_{\text{sem}} K_{\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}}}_{\text{Semantic Path}} + \underbrace{\frac{Q_{\text{geo}} K_{\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}}_{\text{Geometric Path}}
\end{equation}

where:
\begin{align}
    Q_{\text{sem}} &= XW_{Q,\text{sem}}, \quad K_{\text{sem}} = XW_{K,\text{sem}} \quad &(d_{\text{sem}} = 32) \\
    Q_{\text{geo}} &= XW_{Q,\text{geo}}, \quad K_{\text{geo}} = XW_{K,\text{geo}} \quad &(d_{\text{geo}} = 64)
\end{align}

Critically, we apply \textbf{Rotary Position Embeddings (RoPE)} \cite{su2021roformer} \textit{only} to the geometric path:
\begin{equation}
    Q_{\text{geo}}, K_{\text{geo}} \leftarrow \text{RoPE}(Q_{\text{geo}}, K_{\text{geo}}, \text{position})
\end{equation}

The semantic path operates on pure content similarity, while the geometric path encodes positional relationships. The value projection uses the combined dimension:
\begin{equation}
    V = XW_V, \quad W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}
\end{equation}
where $d_{\text{attn}} = d_{\text{sem}} + d_{\text{geo}} = 96$ in our default configuration.

\subsection{The Null Token Mechanism}

Low-rank attention can become unstable when queries lack semantically appropriate keys. We introduce a learnable \textbf{null token} $k_\emptyset$ providing an explicit ``attend nowhere'' option:
\begin{equation}
    \text{Score}_{\text{null}} = \frac{Q_{\text{sem}} k_{\emptyset,\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}} + \frac{Q_{\text{geo}} k_{\emptyset,\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}
\end{equation}

This score is concatenated to the attention matrix before softmax, allowing the model to ``dump'' attention mass when no key is appropriate, which stabilizes training at very low ranks.

\subsection{Tied Q-K Projections}

For the semantic path, we optionally \textbf{tie} the query and key projections: $W_{Q,\text{sem}} = W_{K,\text{sem}}$. This enforces symmetric similarity (``A attends to B iff B attends to A''), which is appropriate for content matching but not for position-dependent relationships.

\subsection{Quantized Inference}

For inference, we apply aggressive quantization to the KV-cache. Recent work has demonstrated that 4-bit KV cache quantization preserves model quality remarkably well. Turboderp's ExLlamaV2 implementation \cite{turboderp2024qcache} showed Q4 cache performs comparably to FP16, and this capability has been integrated into production inference engines like llama.cpp \cite{llamacpp2024kvcache}. We implement block-wise Q4\_0 quantization following this approach:
\begin{equation}
    x_{\text{quantized}} = \text{round}\left(\frac{x}{\text{scale}}\right), \quad \text{scale} = \frac{\max(|x_{\text{block}}|)}{7}
\end{equation}
where each block of 32 elements shares a single FP16 scale factor. Combined with the dimension reduction ($d_{\text{attn}} = 96$ vs $d_{\text{model}} = 512$), this achieves:
\begin{equation}
    \text{Compression} = \underbrace{\frac{512}{96}}_{\text{Dimension}} \times \underbrace{\frac{16}{4}}_{\text{Quantization}} = 5.33 \times 4 \approx 21\times \text{ (per-layer)}
\end{equation}

\paragraph{Calculation of 168$\times$.}
The KV-cache memory at long context depends on the choice of attention dimension $d_{\text{attn}}$ at scale. For a rough Llama-7B-like configuration (32 layers, $d=4096$, 128k context, batch=1), the FP16 KV cache is:
\[
M_{\text{FP16}} \approx 2 \cdot 32 \cdot 4096 \cdot 128\text{k} \cdot 2 \text{ bytes} \approx 64~\text{GiB}.
\]
With 4-bit KV-cache quantization (0.5 bytes/value), the memory becomes:
\[
M_{\text{Q4}} \approx 2 \cdot 32 \cdot d_{\text{attn}} \cdot 128\text{k} \cdot 0.5 \text{ bytes}.
\]
This yields two useful reference scenarios:
\begin{itemize}
    \item \textbf{Constant-fraction $d_{\text{attn}}$ (e.g., $d_{\text{attn}}=768$):} $M_{\text{Q4}} \approx 3.0$~GiB, for an overall reduction of $\sim 21\times$.
    \item \textbf{Fixed-rank $d_{\text{attn}}$ (e.g., $d_{\text{attn}}=96$):} $M_{\text{Q4}} \approx 0.38$~GiB, for an overall reduction of $\sim 168\times$.
\end{itemize}
The architectural contribution is the \textit{dimension reduction} (the ratio $4096/d_{\text{attn}}$); the additional factor of $4\times$ comes from standard 16$\rightarrow$4-bit quantization.

\paragraph{Heterogeneous KV-cache quantization (decoupled).}
A practical benefit of decoupling is that it enables \textit{heterogeneous} KV-cache quantization: we can compress the semantic path more aggressively (e.g., Q4) while keeping the geometric (RoPE) path at higher fidelity (e.g., Q8). As a lightweight sanity check, we compared FP16 caches to a decoupled policy with $K_{\text{sem}}=\text{Q4}$, $K_{\text{geo}}=\text{Q8}$, $V=\text{Q4}$ and a 128-token FP16 residual window. On a small held-out calibration slice from FineWeb-Edu, this policy yielded $\Delta$NLL $\approx 0.015$ nats/token (perplexity ratio $\approx 1.015$) and $\mathrm{KL}(p_{\text{FP16}} \| p_{\text{quant}}) \approx 0.006$ nats/token, while preserving identical greedy decoding for short prompts. These guardrail metrics suggest that aggressive semantic compression can be applied without catastrophic degradation, even though stochastic sampling trajectories may diverge due to small distribution shifts.

While we report training throughput in our experiments, the theoretical FLOPs reduction in the attention mechanism ($O(n^2 d) \to O(n^2 r)$) implies a proportional speedup in the \textit{prefill phase} of inference, where the KV-cache is populated. For autoregressive decoding, the memory bandwidth savings from the smaller cache dominate latency improvements.

% ============================================================================
% 3. EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Why two suites?}
We use a two-stage experimental strategy: the \textbf{v21 ablation suite} (WikiText-2, smaller model) enables rapid iteration and mechanistic insight, while the \textbf{v29 scaling suite} (FineWeb-Edu, larger model) tests whether the same architectural ideas hold at higher scale. Because these suites differ in model size, data distribution, and training details, we report them separately and avoid direct numerical comparison across suites.

\paragraph{Model Configuration.}
We report results from two experiment suites:
\begin{itemize}
    \item \textbf{v21 (ablation suite):} a smaller 6-layer model trained on WikiText-2 for rapid architectural iteration.
    \item \textbf{v29 (scaling suite):} a larger 12-layer model trained on FineWeb-Edu (100M tokens) to validate scaling behavior.
\end{itemize}

\paragraph{Datasets.}
\begin{itemize}
    \item \textbf{WikiText-2}: 2M tokens of Wikipedia text used for rapid prototyping in the v21 ablation suite.
    \item \textbf{FineWeb-Edu}: 100M tokens of educational web content used for the v29 scaling suite. In our implementation, we use a 50,257-token vocabulary (GPT-2 compatible) for tokenized data.
\end{itemize}

\paragraph{Training.}
Unless otherwise noted, models are trained for 6000 steps with gradient clipping at 1.0. Exact hyperparameters and configurations are recorded in the run logs.

\subsection{v21 Ablation Suite: WikiText-2 Results}

\begin{table}[htbp]
\centering
\small
\caption{WikiText-2 Validation Loss Comparison (v21 suite; 6-layer model)}
\label{tab:wikitext_v21}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Attn Config} & \textbf{Params} & \textbf{Val Loss} & \textbf{Tok/s} \\
\midrule
Standard Baseline & $d = 512$ & 31.8M & 5.37 & 20k \\
\textbf{Combined 96} & $d_{\text{attn}} = 96$ & 30.1M & \textbf{5.33} & 117k \\
Bottleneck 128 & $d_{\text{attn}} = 128$ & 31.3M & 5.48 & 128k \\
Decoupled 32/64 & $d_{\text{sem}}{=}32, d_{\text{geo}}{=}64$ & 30.9M & 5.59 & 106k \\
GQA (kv=2) & 8Q/2KV heads & 30.1M & 5.63 & 25k \\
Small Model & $d_{\text{model}} = 128$ & 4.2M & 5.74 & 930k \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding (v21).}
Table~\ref{tab:wikitext_v21} shows that the Combined 96 bottleneck achieves the \textit{lowest} validation loss (5.33), outperforming the full-rank baseline (5.37). This supports the hypothesis that attention routing can be learned in a substantially lower-dimensional interaction space.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/convergence_wikitext.png}}
\caption{Validation loss curves on WikiText-2 (v21 suite). Bottlenecked models converge rapidly; Combined 96 achieves the best final loss.}
\label{fig:convergence_wikitext}
\end{figure}

\subsection{v29 Scaling Suite: FineWeb-Edu Results}

To validate that our findings generalize beyond small datasets, we train on 100M tokens from FineWeb-Edu (Figure~\ref{fig:convergence_fineweb}).

\begin{table}[htbp]
\centering
\small
\caption{FineWeb-Edu Validation Loss (100M tokens, 1024 context, 6000 steps; v29 suite; mean $\pm$ std over two seeds)}
\label{tab:fineweb}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Attn Config} & \textbf{Params} & \textbf{Val Loss} & \textbf{Val PPL} \\
\midrule
Standard Baseline & $d = 512$ & 139.8M & $6.467 \pm 0.242$ & $652.9 \pm 156.2$ \\
GQA (kv=2) & 12Q/2KV & 128.0M & $6.259 \pm 0.062$ & $523.0 \pm 32.4$ \\
Bottleneck (rank 144) & $d_{\text{attn}}=144$ & 116.8M & $6.458 \pm 0.095$ & $639.5 \pm 60.6$ \\
\textbf{Decoupled 48/96} & $d_{\text{sem}}{=}48, d_{\text{geo}}{=}96$ & \textbf{116.8M} & $\mathbf{5.851 \pm 0.038}$ & $\mathbf{347.8 \pm 13.4}$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Scaling Observation.}
On FineWeb-Edu (100M tokens), the Decoupled Bottleneck model \textit{outperformed} the Standard Baseline, a reduced-KV GQA baseline (kv=2), and a matched-rank Bottleneck baseline (Table~\ref{tab:fineweb}). We observe substantial run-to-run variability for the full-rank baseline under this short training regime; therefore, we report mean $\pm$ std over two seeds in Table~\ref{tab:fineweb}, and show representative learning curves for a single seed in Figure~\ref{fig:convergence_fineweb}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1337_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M tokens; seed 1337). Under the same training setup, the Decoupled Bottleneck model converges to the lowest validation loss, outperforming both the standard baseline and GQA.}
\label{fig:convergence_fineweb}
\end{figure}

\subsection{Final Loss Comparison}

Figure~\ref{fig:comparison_bar} provides a direct comparison of final validation losses across our FineWeb-Edu (v29) architectures. The Decoupled Bottleneck (48/96) achieves the best validation loss among baseline, GQA, and bottleneck variants.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1337_comparison_bar.png}}
\caption{Final validation loss comparison (FineWeb-Edu, v29; seed 1337). Lower is better. Decoupled 48/96 achieves the best loss, outperforming both the standard baseline and a GQA baseline (kv=2).}
\label{fig:comparison_bar}
\end{figure}

\subsection{Ablation Studies}

\paragraph{Wide Residual Stream Hypothesis.}
Comparing ``Small Model'' ($d_{\text{model}} = 128$) to ``Bottleneck 128'' ($d_{\text{model}} = 512$, $d_{\text{attn}} = 128$), we observe a 0.26 loss gap (5.74 vs 5.48) and severe overfitting in the small model. This confirms that the \textit{residual stream} must remain wide; only the \textit{attention interaction} can be compressed.

\paragraph{GQA vs. Bottleneck (Head Sharing vs. Interaction Rank).}
We compared our method against Grouped-Query Attention (GQA) with matched KV memory footprints on FineWeb-Edu. While GQA reduces KV cache storage by sharing key-value heads, it retains full-rank query projections and attention scoring in high-dimensional space. In contrast, the Bottleneck architecture reduces the interaction dimension directly. In our v29 runs, both GQA (kv=2) and Bottleneck (rank 144) reduce KV-cache memory to \(\sim 0.75\)~GB and \(\sim 0.84\)~GB at 128k context respectively, but Bottleneck did not consistently improve validation loss over GQA under this recipe (Table~\ref{tab:fineweb}).

\paragraph{Decoupled vs. Bottleneck (Separating Content and Geometry).}
Holding the same total attention dimension fixed ($d_{\text{attn}}=144$), the Decoupled Bottleneck model (48/96) improves validation loss at essentially the same KV cache footprint (\(\sim 0.84\)~GB at 128k context), suggesting that semantic/geometric separation can improve optimization and generalization at this scale (Table~\ref{tab:fineweb}).

\paragraph{Long Context Stability.}
We verify that the geometric path handles extended context correctly:
\begin{itemize}
    \item 1024 context: Val Loss 5.86 (converged smoothly)
    \item 2048 context: Val Loss 6.09 (converged smoothly)
\end{itemize}
The higher loss is expected due to reduced batch size; the key observation is stable training with RoPE on 64 dimensions.

\subsection{Memory-Quality Trade-off}

Figure~\ref{fig:pareto} shows the quality--efficiency trade-off for our FineWeb-Edu (v29) runs. For these experiments, KV-cache memory at 128k context is measured directly from the implementation, and we compare it against the best validation loss achieved by each architecture. Notably, Decoupled 48/96 matches the Bottleneck's KV footprint (same total $d_{\text{attn}}$) while improving quality, and it outperforms GQA despite GQA's smaller KV cache.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1337_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29). Points show best validation loss versus attention dimension ($d_{\text{attn}}$), with annotations derived from measured KV-cache memory at 128k context.}
\label{fig:pareto}
\end{figure}

\subsection{Memory Footprint Analysis}

Table~\ref{tab:memory} shows the KV-cache memory requirements for a 128k context at Llama-7B scale (32 layers, $d = 4096$):

\begin{table}[htbp]
\centering
\small
\caption{KV-Cache Memory for 128k Context (Llama-7B Scale)}
\label{tab:memory}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Architecture} & \textbf{VRAM} & \textbf{Compression} \\
\midrule
Standard (FP16) & 64.0 GB & 1$\times$ \\
GQA 8$\times$ (FP16) & 16.0 GB & 4$\times$ \\
MLA (FP16) & 4.3 GB & 15$\times$ \\
Bottleneck (FP16) & 1.5 GB & 43$\times$ \\
\textbf{Decoupled (Q4, fixed-rank $d_{\text{attn}}{=}96$)} & \textbf{0.38 GB} & \textbf{168$\times$} \\
\textbf{Decoupled (Q4, constant-fraction $d_{\text{attn}}{=}768$)} & \textbf{3.0 GB} & \textbf{21$\times$} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/memory_footprint.png}}
\caption{KV-cache memory comparison at 128k context (illustrative). Depending on whether $d_{\text{attn}}$ scales with $d_{\text{model}}$ or is held fixed, the end-to-end FP16$\rightarrow$Q4 reduction ranges from $\sim$21$\times$ (constant fraction) up to $\sim$168$\times$ (fixed-rank).}
\label{fig:memory}
\end{figure}

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Why Does Low-Rank Attention Work?}

We hypothesize two complementary explanations:

\paragraph{Intrinsic Dimensionality.}
Following Aghajanyan et al. \cite{aghajanyan2021intrinsic}, natural language representations lie on low-dimensional manifolds. The attention mechanism's role is \textit{routing}---selecting which tokens to aggregate---not computing complex transformations. Routing decisions are inherently low-entropy and thus low-rank.

\paragraph{Regularization Effect.}
The bottleneck acts as an implicit regularizer, preventing the model from memorizing spurious token-pair correlations. In our v29 FineWeb runs, reducing the interaction rank improves generalization relative to the full-rank baseline.

\paragraph{Gradient Rank Dynamics.}
AdaRankGrad \cite{refael2024adarankgrad} proves that gradient rank decreases monotonically during training, eventually approaching rank one. This suggests that \textit{architectural} bottlenecks become increasingly appropriate as training progresses---the model naturally ``wants'' to operate in a low-rank subspace. By hard-wiring this constraint from the start, we may accelerate convergence by matching the architecture to the optimization landscape.

\subsection{When to Use Each Architecture}

Our experiments reveal a two-stage story: v21 ablations on WikiText-2 are useful for rapid iteration and mechanistic insight, while v29 FineWeb-Edu results validate the key claims at larger scale.

\begin{itemize}
    \item \textbf{Decoupled Bottleneck:} On FineWeb-Edu (v29), Decoupled 48/96 achieves the best validation loss among the tested variants (baseline, GQA, and bottleneck) while preserving the KV memory benefits of low-rank attention. It is also the \textit{only} architecture enabling \textbf{heterogeneous quantization}---e.g., Q4 for semantic and Q8 for geometric paths---which is critical for long-context inference deployments.
    \item \textbf{Standard Attention:} A strong baseline and simplest implementation, but can be memory-inefficient for long contexts.
\end{itemize}

\paragraph{Recommendation.}
For \textit{training}, use the v21 ablation suite to quickly explore attention-rank and decoupling choices, then validate at scale with the v29 FineWeb suite. For \textit{inference} under memory constraints, convert to Decoupled with heterogeneous quantization (aggressively compress semantic, preserve geometric fidelity).

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Sensitivity to setup:} Results can depend on the training recipe and implementation details. Earlier (v21) FineWeb runs showed a degradation for decoupled attention, whereas the v29 decoupled model improves substantially. This motivates replication across seeds, longer training, and additional datasets.
    \item Experiments are limited to 512-dim models. Verification at 7B+ scale is needed.
    \item The optimal $(d_{\text{sem}}, d_{\text{geo}})$ split may vary with model scale.
    \item We have not evaluated on downstream tasks (e.g., MMLU, HellaSwag).
    \item Throughput measurements are from training; inference latency benchmarks are future work.
\end{itemize}

% ============================================================================
% 5. CONCLUSION
% ============================================================================
\section{Conclusion}

We have demonstrated that attention in Transformers contains significant redundancy. On FineWeb-Edu (100M tokens), the Decoupled Bottleneck architecture \textbf{surpassed} the standard baseline, a GQA baseline (kv=2), and a matched-rank Bottleneck baseline under the same training recipe (Table~\ref{tab:fineweb}). These results suggest that, at this scale and training horizon, standard high-dimensional attention can be over-parameterized; larger-scale validation and longer training are important future work.

The core insight is architectural: \textbf{Attention is a router, not a processor.} The heavy computation should happen in the feedforward layers (which we leave at full rank), while attention merely selects which tokens to aggregate. By matching the architecture to this functional role, we unlock dramatic efficiency gains.

Our Decoupled Bottleneck Attention separates semantic matching from positional geometry, allowing aggressive compression on the former while preserving RoPE fidelity on the latter. Combined with 4-bit quantization, this enables 128k-context inference on consumer hardware (Figure~\ref{fig:memory})---transforming a datacenter problem into a laptop-solvable one.

\paragraph{Future Work.}
We plan to: (1) validate at 7B+ scale where the efficiency gains compound; (2) explore learned mixing weights between semantic and geometric paths; (3) test robustness across seeds, longer training, and additional datasets to understand when decoupling improves optimization; and (4) benchmark inference latency on production hardware.

% ============================================================================
% STATEMENTS
% ============================================================================
\section*{Statements and Declarations}

\paragraph{Conflict of Interest.}
The author declares no competing interests. This research was conducted independently without corporate affiliation or funding from entities with financial interests in the outcomes.

\paragraph{Data Availability.}
All datasets used in this study are publicly available: WikiText-2 \cite{merity2016pointer} is available from Salesforce Research, and FineWeb-Edu is available from Hugging Face. The code, trained model checkpoints, and all experimental logs are available at \url{https://github.com/theapemachine/experiments}.

\paragraph{Funding.}
This research was conducted without external funding. All computational resources were provided by the author.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Replication Plots for FineWeb-Edu (v29; Seed 1338)}
\label{app:v29_seed1338}

Table~\ref{tab:fineweb} reports mean $\pm$ std over two seeds. In the main text, we show representative learning curves and summary plots for seed 1337; here we include the corresponding plots for seed 1338 to make the replication evidence explicit.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1338_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M tokens; seed 1338).}
\label{fig:convergence_fineweb_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1338_early_convergence.png}}
\caption{Early convergence view on FineWeb-Edu (seed 1338), highlighting the first phase of optimization.}
\label{fig:early_convergence_fineweb_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1338_comparison_bar.png}}
\caption{Final validation loss comparison on FineWeb-Edu (v29; seed 1338). Lower is better.}
\label{fig:comparison_bar_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1338_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29; seed 1338).}
\label{fig:pareto_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/m4max_seed1338_kv_memory_128k.png}}
\caption{Measured KV-cache memory at 128k context (v29; seed 1338).}
\label{fig:kv_memory_128k_seed1338}
\end{figure}

\section{Effective Rank Evidence (Projected \texorpdfstring{$W_Q/W_K$}{WQ/WK})}
\label{app:effective_rank}

We compute singular value spectra and entropy-based effective rank for the $W_Q$/$W_K$ projection matrices in trained checkpoints. The script to reproduce this figure is included in the repository; the figure below is generated automatically when that script is run.

\begin{figure}[htbp]
\centering
\IfFileExists{assets/m4max_rank_evidence.png}{
  \includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_rank_evidence.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{assets/m4max_rank_evidence.png}.\\
  Run the rank analysis script to generate this figure.}}
}
\caption{Singular value spectra and entropy-based effective rank for $W_Q$ and $W_K$ across layers (example: v29 checkpoints).}
\label{fig:rank_evidence}
\end{figure}

\section{Long-Context Sanity Checks}
\label{app:long_context}

To complement the theoretical KV-cache scaling analysis, we include two lightweight long-context probes: (1) teacher-forced loss at increasing context lengths (RoPE extrapolation), and (2) a passkey ``needle-in-a-haystack'' next-token retrieval probe. These are not full downstream evaluations, but they help catch obvious long-context regressions and RoPE failures.

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_rope_extrapolation.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_rope_extrapolation.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \texttt{assets/m4max\_baseline\_rope\_extrapolation.png}}}
}
\caption{Baseline}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_rope_extrapolation.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_rope_extrapolation.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \texttt{assets/m4max\_decoupled\_rope\_extrapolation.png}}}
}
\caption{Decoupled}
\end{subfigure}
\caption{RoPE extrapolation probe (teacher-forced). Figures are generated by \texttt{test\_rope\_extrapolation\_v29.py}.}
\label{fig:rope_extrapolation_appendix}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \texttt{assets/m4max\_baseline\_needle\_haystack.png}}}
}
\caption{Baseline}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \texttt{assets/m4max\_decoupled\_needle\_haystack.png}}}
}
\caption{Decoupled}
\end{subfigure}
\caption{Passkey needle-in-a-haystack next-token probe. Figures are generated by \texttt{test\_needle\_haystack\_v29.py}.}
\label{fig:needle_haystack_appendix}
\end{figure}

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plain}
\bibliography{references}

\end{document}
