_wandb:
    value:
        cli_version: 0.23.1
        e:
            g7h0ozla0vg0lcbsvnwux1j4p6auky21:
                apple:
                    ecpuCores: 4
                    gpuCores: 40
                    memoryGb: 128
                    name: Apple M4 Max
                    pcpuCores: 12
                    ramTotalBytes: "137438953472"
                    swapTotalBytes: "4294967296"
                args:
                    - --mode
                    - train
                    - --size
                    - 1b
                    - --exp
                    - train_decoupled_fast
                    - --data
                    - fineweb_1b.npy
                    - --data-format
                    - npy
                    - --vocab-size
                    - "50257"
                    - --block
                    - "4096"
                    - --train-seq-len
                    - "2048"
                    - --seq-schedule
                    - 512@0,1024@200,2048@600,3072@1200,4096@2000
                    - --steps
                    - "20000"
                    - --log-every
                    - "10"
                    - --eval-every
                    - "200"
                    - --eval-iters
                    - "5"
                    - --eval-seq-len
                    - "512"
                    - --analysis-every
                    - "0"
                    - --instrument
                    - basic
                    - --batch-size
                    - "8"
                    - --grad-accum
                    - "8"
                    - --wandb
                    - --wandb-entity
                    - p4n0p71c0n
                    - --wandb-project
                    - 1b_decoupled
                    - --wandb-name
                    - 1b_decoupled
                    - --wandb-mode
                    - online
                cpu_count: 16
                cpu_count_logical: 16
                disk:
                    /:
                        total: "1995218165760"
                        used: "1048040198144"
                email: theapemachine@gmail.com
                executable: /Users/theapemachine/go/src/github.com/theapemachine/experiments/.venv/bin/python3.12
                git:
                    commit: 51037f66d02d44b914aa82a3f0ac0b76e4a07451
                    remote: https://github.com/TheApeMachine/experiments.git
                host: marvin-2.localdomain
                memory:
                    total: "137438953472"
                os: macOS-15.6-arm64-arm-64bit
                program: -m production.cli
                python: CPython 3.12.12
                root: runs/1b_train_decoupled_fast
                startedAt: "2025-12-20T09:59:02.905135Z"
                writerId: g7h0ozla0vg0lcbsvnwux1j4p6auky21
        m: []
        python_version: 3.12.12
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
                - 61
            "4": 3.12.12
            "5": 0.23.1
            "12": 0.23.1
            "13": darwin-arm64
args:
    value:
        adam_betas: 0.9,0.95
        adam_eps: 1e-08
        amp: true
        amp_dtype: bf16
        analysis_every: 0
        analysis_heads: "0"
        analysis_layers: 0,-1
        analysis_local_window: 32
        analysis_max_tokens: 256
        analysis_save_scores: false
        analysis_topk: 8
        attn_dim: 768
        attn_mode: decoupled
        batch_by_seq: null
        batch_schedule: null
        batch_size: 8
        block: 4096
        ckpt: null
        compile: true
        compile_mode: reduce-overhead
        d_ff: 8192
        d_model: 2048
        data: fineweb_1b.npy
        data_dtype: uint16
        data_format: npy
        device: null
        draft_ckpt: null
        dropout: 0
        embed_dim: 2048
        eval_every: 200
        eval_iters: 5
        eval_seq_len: 512
        exp: train_decoupled_fast
        geo_dim: 512
        grad_accum: 8
        grad_checkpoint: true
        grad_clip: 1
        instrument: basic
        kv_cache: fp16
        kv_cache_k: null
        kv_cache_k_geo: null
        kv_cache_k_sem: null
        kv_cache_v: null
        kv_decode_block: 1024
        kv_fused: auto
        kv_head: null
        kv_qblock: 32
        kv_qblock_k: null
        kv_qblock_k_geo: null
        kv_qblock_k_sem: null
        kv_qblock_v: null
        kv_residual: 128
        layers: 18
        legacy_micro_steps: false
        lion_betas: 0.9,0.99
        live: auto
        live_plot: false
        live_update_every: 1
        log_every: 10
        lr: 0.0001
        lr_schedule: cosine
        matmul_precision: high
        max_new_tokens: 50
        min_lr: 0
        mlp: swiglu
        mode: train
        n_head: 16
        nan_lr_decay: 0.5
        nan_policy: reduce_lr
        no_amp: false
        no_compile: false
        no_decoupled_gate: false
        no_grad_checkpoint: false
        no_learned_temp: false
        no_null_attn: false
        no_rope: false
        no_tie_qk: false
        null_attn: false
        opt_foreach: false
        opt_fused: false
        optimizer: lion
        out_dir: runs/1b_train_decoupled_fast
        param_dtype: bf16
        print_config: false
        prompt_tokens: "0"
        rope: false
        rope_base: 10000
        run_root: runs
        run_tag: null
        save_every: 0
        seed: 1337
        self_opt: none
        self_opt_block_n: "128"
        self_opt_cache: null
        self_opt_calib_decode: 32
        self_opt_calib_prefill: 128
        self_opt_calib_tokens: null
        self_opt_decode_blocks: 256,512,1024,2048
        self_opt_hysteresis: 0.03
        self_opt_interval: 256
        self_opt_iters: 3
        self_opt_k_geo_kinds: q8_0,q4_0,fp16
        self_opt_k_sem_kinds: q4_0,nf4,q8_0,fp16
        self_opt_layerwise_cache: false
        self_opt_mem_budget_mb: null
        self_opt_mem_overhead_frac: 0.1
        self_opt_policy_hysteresis: 0.02
        self_opt_policy_iters: 3
        self_opt_policy_prefix_len: null
        self_opt_policy_quality: false
        self_opt_policy_warmup: 1
        self_opt_prefer_low_mem_within: 0.02
        self_opt_qblocks: 16,32,64
        self_opt_quality_delta_nll_tol: 0.02
        self_opt_quality_kl: false
        self_opt_quality_kl_tol: null
        self_opt_quality_ppl_ratio_tol: 1.02
        self_opt_quality_tol: 0.5
        self_opt_residuals: 0,32,64,128
        self_opt_scope: all
        self_opt_stages: 2,3
        self_opt_v_kinds: q4_0,q8_0,fp16
        self_opt_verbose: false
        self_opt_verify: false
        self_opt_verify_tol: 0.005
        self_opt_warmup: 1
        self_opt_warps: 4,8
        sem_dim: 256
        seq_schedule: 512@0,1024@200,2048@600,3072@1200,4096@2000
        size: 1b
        spec_disable_below_accept: 0
        spec_extra_token: false
        spec_k: 4
        spec_method: reject_sampling
        steps: 20000
        sync_timing: false
        tb: false
        temperature: 1
        tie_qk: true
        tokenizer: word
        top_k: null
        train_autotune: "off"
        train_autotune_batch_sizes: 8,16,24,32,48,64
        train_autotune_gbs: 64
        train_autotune_iters: 1
        train_autotune_max_driver_gb: 0
        train_autotune_seq_lens: "512"
        train_autotune_warmup: 0
        train_seq_len: 2048
        val_frac: 0.1
        vocab_size: 50257
        wandb: true
        wandb_entity: p4n0p71c0n
        wandb_group: null
        wandb_mode: online
        wandb_name: 1b_decoupled
        wandb_project: 1b_decoupled
        wandb_tags: null
        warmup_steps: 200
        weight_decay: 0.1
config:
    value:
        attn_dim: 768
        attn_mode: decoupled
        block_size: 4096
        d_ff: 8192
        d_model: 2048
        decoupled_gate: true
        dropout: 0
        embed_dim: 2048
        geo_dim: 512
        kv_head: null
        learned_temp: true
        mlp: swiglu
        n_head: 16
        n_layer: 18
        null_attn: false
        rope: true
        rope_base: 10000
        sem_dim: 256
        tie_qk: true
        vocab_size: 50257
