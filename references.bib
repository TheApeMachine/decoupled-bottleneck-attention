@article{refael2024adarankgrad,
  title={AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning},
  author={Refael, Yehonathan and Svirsky, Jonathan and Shustin, Boris and Huleihel, Wasim and Lindenbaum, Ofir},
  journal={arXiv preprint arXiv:2410.17881},
  year={2024}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and others},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{bhojanapalli2020lowrank,
  title={Low-Rank Bottleneck in Multi-Head Attention Models},
  author={Bhojanapalli, Srinadh and others},
  journal={ICML},
  year={2020}
}

@article{deepseek2024v2,
  title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{park2018bam,
  title={BAM: Bottleneck Attention Module},
  author={Park, Jongchan and others},
  journal={BMVC},
  year={2018}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and others},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{aghajanyan2021intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  journal={ACL},
  year={2021}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{kobayashi2024weightdecay,
  title={Weight Decay Induces Low-Rank Attention Layers},
  author={Kobayashi, Seijin and von Oswald, Johannes and Sacramento, Jo\~{a}o},
  journal={NeurIPS},
  year={2024}
}

@article{he2020deberta,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={ICLR},
  year={2021}
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr\'{o}n, Federico and Sanghai, Sumit},
  journal={EMNLP},
  year={2023}
}

@article{shazeer2019mqa,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{chiang2025rope,
  title={The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval},
  author={Chiang, David and Yogatama, Dani},
  journal={arXiv preprint},
  year={2025}
}

@article{chen2024dha,
  title={DHA: Decoupled-Head Attention for Efficient KV Cache Compression},
  author={Chen, Lu and others},
  journal={arXiv preprint},
  year={2024}
}

@article{transmla2025,
  title={TransMLA: Multi-Head Latent Attention Transfer for Efficient KV Cache Compression},
  author={Various Authors},
  journal={arXiv preprint},
  year={2025}
}

@article{mha2mla2025,
  title={MHA2MLA: Converting Multi-Head Attention to Multi-Head Latent Attention},
  author={Various Authors},
  journal={arXiv preprint},
  year={2025}
}

@article{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@misc{turboderp2024qcache,
  title={Quantized KV Cache Evaluation},
  author={Turboderp},
  howpublished={\url{https://github.com/turboderp/exllamav2/blob/master/doc/qcache_eval.md}},
  year={2024},
  note={ExLlamaV2 implementation showing Q4 cache matches FP16 quality}
}

@misc{llamacpp2024kvcache,
  title={4-bit KV Cache Implementation},
  author={Gerganov, Georgi and others},
  howpublished={\url{https://github.com/ggml-org/llama.cpp/pull/7412}},
  year={2024},
  note={llama.cpp PR\#7412: Production Q4\_0/Q8\_0 KV cache support}
}

@article{hooper2024kvquant,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and others},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{wang2025lowdim,
  title={Attention Layers Add Into Low-Dimensional Residual Subspaces},
  author={Wang, Yizhou and others},
  journal={arXiv preprint},
  year={2025},
  note={Shows attention outputs are approximately 60\% low-dimensional}
}

@article{merity2016pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016},
  note={Introduces WikiText-2 and WikiText-103 benchmarks}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer},
  author={Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{zaheer2020bigbird,
  title={Big Bird: Transformers for Longer Sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@inproceedings{choromanski2021performer,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, {\L}ukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  booktitle={ICLR},
  year={2021},
  note={arXiv:2009.14794}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle={ICLR},
  year={2020},
  note={arXiv:2001.04451}
}

@inproceedings{rae2020compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling},
  author={Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
  booktitle={ICLR},
  year={2020},
  note={arXiv:1911.05507}
}

@article{huang2023longcontextsurvey,
  title={Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey},
  author={Huang, Yunpeng and Xu, Jingwei and Lai, Junyu and Jiang, Zixu and Chen, Taolue and Li, Zenan and Yao, Yuan and Ma, Xiaoxing and Yang, Lijuan and Chen, Hao and Li, Shupeng and Zhao, Penghao},
  journal={arXiv preprint arXiv:2311.12351},
  year={2023}
}

@inproceedings{kuang2025structuredmatrices,
  title={Customizing the Inductive Biases of Softmax Attention using Structured Matrices},
  author={Kuang, Yilun and Amsel, Noah and Lotfi, Sanae and Qiu, Shikai and Potapczynski, Andre and Wilson, Andrew Gordon},
  booktitle={ICML},
  year={2025},
  note={OpenReview: \url{https://openreview.net/forum?id=Roc5O1ECEt}}
}

@inproceedings{amsel2025qualityheads,
  title={Quality over Quantity in Attention Layers: When Adding More Heads Hurts},
  author={Amsel, Noah and Yehudai, Gilad and Bruna, Joan},
  booktitle={ICLR},
  year={2025},
  note={OpenReview: \url{https://openreview.net/forum?id=y9Xp9NozPR}}
}

@article{li2025commvq,
  title={CommVQ: Commutative Vector Quantization for KV Cache Compression},
  author={Li, Junyan and Zhang, Yang and Hasan, Muhammad Yusuf and Chafekar, Talha and Cai, Tianle and Ren, Zhile and Guo, Pengsheng and Karimzadeh, Foroozan and Reed, Colorado and Wang, Chong and Gan, Chuang},
  journal={arXiv preprint arXiv:2506.18879},
  year={2025},
  note={ICML 2025 poster}
}