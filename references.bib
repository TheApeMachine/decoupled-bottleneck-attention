@article{refael2024adarankgrad,
  title={AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning},
  author={Refael, Yehonathan and Svirsky, Jonathan and Shustin, Boris and Huleihel, Wasim and Lindenbaum, Ofir},
  journal={arXiv preprint arXiv:2410.17881},
  year={2024}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and others},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{bhojanapalli2020lowrank,
  title={Low-Rank Bottleneck in Multi-Head Attention Models},
  author={Bhojanapalli, Srinadh and others},
  journal={ICML},
  year={2020}
}

@article{deepseek2024v2,
  title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{park2018bam,
  title={BAM: Bottleneck Attention Module},
  author={Park, Jongchan and others},
  journal={BMVC},
  year={2018}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and others},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{aghajanyan2021intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  journal={ACL},
  year={2021}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{kobayashi2024weightdecay,
  title={Weight Decay Induces Low-Rank Attention Layers},
  author={Kobayashi, Seijin and von Oswald, Johannes and Sacramento, Jo\~{a}o},
  journal={NeurIPS},
  year={2024}
}

@article{he2020deberta,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={ICLR},
  year={2021}
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr\'{o}n, Federico and Sanghai, Sumit},
  journal={EMNLP},
  year={2023}
}

@article{shazeer2019mqa,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{chiang2025rope,
  title={The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval},
  author={Chiang, David and Yogatama, Dani},
  journal={arXiv preprint},
  year={2025}
}

@article{chen2024dha,
  title={DHA: Decoupled-Head Attention for Efficient KV Cache Compression},
  author={Chen, Lu and others},
  journal={arXiv preprint},
  year={2024}
}

@article{transmla2025,
  title={TransMLA: Multi-Head Latent Attention Transfer for Efficient KV Cache Compression},
  author={Various Authors},
  journal={arXiv preprint},
  year={2025}
}

@article{mha2mla2025,
  title={MHA2MLA: Converting Multi-Head Attention to Multi-Head Latent Attention},
  author={Various Authors},
  journal={arXiv preprint},
  year={2025}
}

@article{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@misc{turboderp2024qcache,
  title={Quantized KV Cache Evaluation},
  author={Turboderp},
  howpublished={\url{https://github.com/turboderp/exllamav2/blob/master/doc/qcache_eval.md}},
  year={2024},
  note={ExLlamaV2 implementation showing Q4 cache matches FP16 quality}
}

@misc{llamacpp2024kvcache,
  title={4-bit KV Cache Implementation},
  author={Gerganov, Georgi and others},
  howpublished={\url{https://github.com/ggml-org/llama.cpp/pull/7412}},
  year={2024},
  note={llama.cpp PR\#7412: Production Q4\_0/Q8\_0 KV cache support}
}

@article{hooper2024kvquant,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and others},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{wang2025lowdim,
  title={Attention Layers Add Into Low-Dimensional Residual Subspaces},
  author={Wang, Yizhou and others},
  journal={arXiv preprint},
  year={2025},
  note={Shows attention outputs are approximately 60\% low-dimensional}
}

@article{merity2016pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016},
  note={Introduces WikiText-2 and WikiText-103 benchmarks}
}